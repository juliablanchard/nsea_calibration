---
title: 'From Steady State To Changes Through Time'
author: "Julia Blanchard"
date: "22/08/2022"
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Three Stages

In a previous blog post by Gustav Delius, 3 stages of building mizer
models are described:

1.  Collect information about the important species in your ecosystem
    and how they are fished. This includes physiological parameters for
    the species as they might be found on FishBase, but also information
    about how abundant the species are and how they are being fished.

2.  Create a mizer model that in its steady state reproduces the
    time-averaged observed state of your fish community.

3.  Tune the model parameters further to also reproduce time-series
    observations that capture some of the system's sensitivity to
    perturbations, like changes in fishing pressure.

This blog post will focus on the transition between stage 2 and 3.
First, we wil use Gustav's recipe (or constant reproduction trick) to
get some initial steady state estimates, which I then use to

# Example we will use

We will re-visit stage 2 first, this time using only catch data which is
often the only available data source for some regions of the world.
Previously the model was calibrated with catches and biomasses, the
latter from stock assessments. These are not a perfect choice, being
derived from a combination of data and single species models. However,
they are often the best information available. As inputs to force the
model through time we explore two options: forcing with fishing
mortality rates (again from stock assessments) or estimate fishing
mortality rates using a logistic function of fishing mortality through
time (as in this paper).

Some of the functions we will be using are still in active development
in the [mizerExperimental](https://sizespectrum.org/mizerExperimental/)
and mizerHowTo packages. Therefore we will always want to make sure we
are loading the latest version of the package with

```{r}
#remotes::install_github("sizespectrum/mizerExperimental")
library(mizerExperimental)
#remotes::install_github("sizespectrum/mizerHowTo")
library(mizerHowTo)
library(tidyverse)
library(parallel)
library(pbapply)
library(lhs)
library(tgp)

```

This blog post was compiled with mizer version
`r packageVersion("mizer")` and mizerExperimental version
`r packageVersion("mizerExperimental")`

We will use the North Sea model parameters which includes time-averaged
biomass and yield observations needed to carry out this example.

```{r, code_folding=TRUE, layout="l-body-outset"}
species_params <- readRDS("data-for-steady-to-time/ns_calib_param.RDS")
```

# Model criteria

Before we look at the calibration steps we want to ensure we have some
clear criteria. These are:

1.  Modelled yields at steady state are within +/- 10% of the observed
    time-averaged yields.

2.  Unfished normalised biomass size spectrum slope that is negative and
    close to -1

3.  Growth curves roughly approximate the von Bertalanffy growth curves
    for each species (noting in reality these curves are higly
    variable).

4.  Diets capture ontogenetic changes with body size with largest
    species becoming more piscivorous at larger sizes.

5.  Recruitment parameters ensured single-species yield curves were
    dome-shaped, as expected by theory.

6.  The modelled catches through time captured the trends in the
    reported catches through time.

For the final two criteria we need to examine the dynamics aspects of
the model in terms of fishing. Before we do that let's check some of the
criteria for the steady state of this model.

# A recap on getting the steady state

There are many different ways to calibrate or tune models using
time-averaged data. I usually do this in mizer by running the dynamics
through time, outputting the time-averaged biomass or catches and
calculating the differences with observations using a least-squares
approach (usually optimisation, not always). This usually comes with
challenges,which I won't go into here, but it can get messy.

So I was excited to check out the constant reproduction trick, which I
think could bypass a lot of fiddling around and saves the optimisation
step for stage 3: fitting to times series data - when more data comes
into play and we will get onto a little later.

The first step of stage 2 is very handy. Instead of setting up the model
and projecting through time, we will use `steady()` to get the steady
state with constant reproduction (and the adjusted the reproduction
parameters, the reproductive efficiencies or erepro). It then sets the
resulting steady state as the initial state of the MizerParams object.

This time we also use the species x specie interaction matrix, which was
based on the spatial co-occurrences of each species, hence influences
the strength of encounter interactions.

```{r,code_folding=TRUE,echo = F,message=F, warning=F}
gear_params<-data.frame(species = species_params$species,
               gear = species_params$species,
               sel_func = "sigmoid_length",
               l25 =  c(7.6, 9.8, 8.7, 10.1, 11.5, 19.8, 16.4, 19.8, 11.5,
                        19.1, 13.2, 35.3),
               l50 = c(8.1, 11.8, 12.2, 20.8, 17.0, 29.0, 25.8, 29.0, 17.0,
                       24.3, 22.9, 43.6),
               catchability = rep(1,dim(species_params)[1]))

params <- newMultispeciesParams(species_params = species_params,gear_params=gear_params,inter,initial_effort = species_params$catchability,max_w=1e6) # inter comes with loading "mizer")
species_params(params)$gear<-species_params(params)$species
params <- steady(params)
plotlySpectra(params, power = 2)
```

We can project the new params object to steady state and then check it.

```{r,code_folding=TRUE}
sim <- project(params,effort=initial_effort(params), t_max = 100)
plot(sim)
gear_params(params)
```

This shows there is a steady state with coexistence for all species,
reasonable feeding level and size spectrum. How well do the yields match
the observations?

Using the handy function 'calibrateYield' scales the system (background
resource and other linked parameters) to get the total biomass in the
model to match up with the total observed biomass of all 12 species.
Then 'matchYield' calculates the scaling needed for each of the species'
yields to match up to their observed ones. This part is a bit of an
iterative process: match yield, calculate steady steady, re-check match
to yield and repeat until satisfactory. It uses %\>% from tidyverse to
make this easy.

I repeated the below 3 times, which gives a total relative error of
0.918.

```{r}
params <- params %>% calibrateYield() %>%  matchYields() %>%  steady()
plotYieldObservedVsModel(params,ratio=T)
```

Pretty good! Now let's check we are at steady state and have a look to
see if the size spectra look OK:

```{r,code_folding=TRUE}
sim <- project(params,effort=initial_effort(params),t_max = 10)
plotlySpectra(sim, power = 2, total = TRUE)
plot(sim)
```

The diagnostic plots look reasonable as do the growth curves below, in
comparison with those based purely on the empirical von Bertlanffy
parameters. We would of course expect some variation here, with the
modelling being driven by mechanistic processes involved in food
dependent growth and size-at-age data typically highly variable.
Ultimately, we need to re-examine these later with data as well.

```{r}
plotGrowthCurves(sim, species_panel = TRUE)
```

We also want to check the diets look OK. For this we can use a handy
plot in 'mizerHowTo', which is a package that has more in depth
tutorials for you to play with some day, particularly for examining
these diagnostics further and for hand-tuning, using shiny apps.

```{r}
plotDiet2(sim)
```

The initial "effort" values used in the previous version of this model
are actually fishing mortality rates from single species stock
assessments which are set by assuming catchability of 1. We could use
this model as a starting point to estimate the levels of fishing
mortality rates that produce the catches through time (as in
[this](https://www.pnas.org/doi/abs/10.1073/pnas.1612722114) paper).
More on that later.

We do have a steady state that we can use as a starting point for tuning
the model through time. Before we examine time-series we need to
consider how the density dependence is set up in this model [density
dependence in
reproduction](https://blog.mizer.sizespectrum.org/posts/2021-08-03-density-dependence-in-reproduction/).

The level of density dependence can have a big effect on the sensitivity
of each species to fishing.

# Setting up density dependence in reproduction

In the above steps we have not checked what the additional sources of
density dependence not fully captured by the model are set to. We can
examine this from:

```{r,code_folding=TRUE}
reproduction_level <- getReproductionLevel(params)
reproduction_level
```

And we can double check to see what the Rmax and erepro values, for each
species:

```{r,code_folding=TRUE}
species_params(params)[, c("erepro", "R_max")]
```

There seems to be a small amount of density dependence for some species
but not others.

Additional unaccounted for density dependence could include habitat, are
extra sources of mortality or reproductive success. These parameters
along with erepro, the reproductive efficiencies, affect how species
collapse (and recover) under changes in fishing through time.

We may wish to not include any additional density dependence, or we may
wish to find out, what the best estimates of these values are, when the
model is fit to time series data. This is what we will do below, this
time using catch data through time, and additonally estimating two
parameters associated with the trajectory of fishing mortality through
time. In a later post I will show how we could use effort information if
that is available or catch only. For now, this suits our purpose.

First, we need to assume some level of density dependence to get
started. Arbritralily I will choose 0.9 to get some values, and we can
also have a look at how this appears in the stock-recruitment function:

```{r,code_folding=TRUE}
params2 <- setBevertonHolt(params, reproduction_level = 0.9)

plotBevertonHolt2 <- function(params, params2, species) {
  select <- species_params(params)$species == species
  erepro <- species_params(params)$erepro[select]
  w0 <- params@w[params@w_min_idx[select]]
  E_R_ss <- getRDI(params)[select] / erepro * 2 * w0
  R_dd_ss <- getRDD(params)[select]
  E_R <- seq(0, 2 * E_R_ss, length.out = 50)
  
  R_max  <- species_params(params)$R_max[select]
  R_di = erepro * E_R / 2 / w0
  R_dd <- R_di / (1 + R_di / R_max)
  df <- melt(data.frame(E_R, R_dd, R_di, R_max), id.vars = "E_R")
  df$Model <- "Model 1"
  
  erepro <- species_params(params2)$erepro[select]
  R_max  <- species_params(params2)$R_max[select]
  R_di = erepro * E_R / 2 / w0
  R_dd <- R_di / (1 + R_di / R_max)
  df2 <- melt(data.frame(E_R, R_dd, R_di, R_max), id.vars = "E_R")
  df2$Model <- "Model 2"
  
  ggplot(rbind(df, df2)) +
    geom_line(aes(x = E_R, y = value, linetype = variable,
                  colour = Model, size = Model)) +
    geom_point(aes(x = E_R_ss, y = R_dd_ss), size = 3, color = "red") +
    ylim(NA, 1.1 * R_max) +
    ylab("Reproduction rate [eggs/year]") +
    xlab("Energy invested [g/year]") +
    labs(linetype = "", size = "R_max", colour = "R_max") +
    scale_size_manual(values = c(0.5, 1)) +
    scale_colour_manual(values = c("blue", "black")) +
    scale_linetype_manual(values = c("solid", "dashed", "dotted"))
}

plotBevertonHolt2(params, params2, "Cod")
plotBevertonHolt2(params, params2, "Sprat")


```

Ideally, using the level of density dependence we think should be
correct the yield curves would look dome-shaped and consistent with
theory. But how do we know what level of density dependence to set in
real ecosystems?

An alternative approach could be to confront the model with time series
data of catches, and to estimate the reproduction parameters that way.

# Forcing the model through time

Next, we will force the model through time with changes in fishing
mortality. These are fairly recent ICES stock assessment fishing
mortality estimates, here used as model inputs through time.

Let's incorporate this time-varying fishing into the model. Since the
time series goes back further than the years for which the time-averaged
model was calibrated, we will set the initial value to an unfished
steady state. This initial state turns out to be very important for how
well the time series fits and more advanced statistical studies estmate
the initial fishing mortality rate.

Let's incorporate this time-varying fishing into the model.

```{r,code_folding=TRUE}
# read in stored fishing mortality (here called effort) time series
effort <- readRDS("data-for-steady-to-time/effortTime.RDS")
# lets start the model off with the level of fishing using the first year of the input values
params2<- projectToSteady(params2, effort = effort[1,],t_max = 200)

simt<- project(params2, effort = effort)
plotlyYield(simt)
plotlyBiomass(simt)
```

Here, we are interested in examining the changes along side
observations. Let's read in the observed landings for the North Sea and
add these to our plot.

```{r,warning=FALSE}
source("calibration_functions.R")

yields_obs <-readRDS("yields_time.RDS")
names(yields_obs)<-c("Year","Species","Yield")

plotYield(simt) +    facet_wrap(~Species,scales="free_y")+
geom_point(data=yields_obs,aes(x = Year, y = (Yield), colour = Species),size=0.1)


```

Currently, for some species we can see the modeled yields are not near
the data time series, but others look broadly ok. Also the stocks seem
to collapse earlier in the model than the data. Remember we arbitrarily
set the density dependence for all species to 0.9.

If we look at the parameters without density dependence we can see
stocks collapsed even earlier.

```{r}
params<- projectToSteady(params, effort = effort[1,],t_max = 200)
simt2<- project(params, effort = effort)

plotYield(simt2) +    facet_wrap(~Species,scales="free_y")+
geom_point(data=yields_obs,aes(x = Year, y = (Yield), colour = Species),size=0.1)


```

# Using the time series data to estimate reproduction parameters

What would the reproduction parameters need to be in order to fit the
overall time series data better?

One issue could be that the erepro values we just re-calibrated the
model make the species much more reactive to fishing. Let's examine how
sensitive the time series (and their visual agreement to data look when
we change our assumptions about eRepro, and possibly Rmax).

First let's just see what happens if we reset the reproduction level
directly for a species.

```{r}
paramsRepro <- setBevertonHolt(params2, reproduction_level = c("Cod" = 0.9))
paramsRepro<- projectToSteady(paramsRepro, effort = effort[1,])
simt4<- project(paramsRepro , effort = effort)

plotYield(simt4,species="Cod") +    facet_wrap(~Species,scales="free_y")+
geom_point(data=filter(yields_obs,Species=="Cod"),aes(x = Year, y = (Yield), colour = Species),size=0.1)

```

Here we can see that reducing the reproduction level does have a big
effect on when the stocks start collapsing, as expected. And having the
values low doesn't seem consistent with the data.

Given we have much more data now than in the time-averaged calibration
we can estimate more parameters. Because erepro influences how stocks
respond to fishing this is an appropriate one to include, but only when
Rmax is also being re-calibrated.

Other parameters could be estimated here, such as the kappa and r_pp to
control the background resource spectrum or initial and maximum fishing
mortality parameters. Bayesian parameter uncertainty work has done this
with Rmax (Spence et al. 2016) and time-varying Fs (Spence et al 2021)
using this North Sea model. The first step of those analyses involved
reducing the sampling space and is called "history matching". This
allows us to identify parameters that are close and could be used for
further Bayesian or optimisation routines. We start with a range of
parameter values and set up a "latin hypercube" sampling scheme to
ensure a good spread of these. Then we can run these values very quickly
using a package called *pbapply*.

Because running so many iterations of the model is computationally
intensive so we will run the below setting up multiple computer cores
and using *parallel*. The function *getErrorTime* is user defined
function we want to minmise the error. The runs the model and calculates
the sum of squared errors between the observed and modeled catches for
all species' time series.

We can alternatively do this with latin hypercube sampling and pbapply
,which has a nice progress bar,

```{r,code_folding=TRUE,eval=F}
# set up workers
  noCores <- parallel::detectCores() - 1 # keep some spare core
  cl <- parallel::makeCluster(noCores, setup_timeout = 0.5)
  setDefaultCluster(cl = cl)
  clusterExport(cl, varlist = "cl",envir=environment())
  clusterEvalQ(cl, {
    library(mizerExperimental)
    library(optimParallel)
  })
  
num_species<-12
# num runs
num_iter=1000
# set the upper and lower bounds for each variable
rect_vary<-cbind(rep(0.5,num_species),rep(0.9999,num_species))
set.seed(123)
parameters_new <- data.frame(lhs(num_iter, rect_vary))
names(parameters_new)<-row.names(species_params(params))

parameters_new$error<-pbapply(parameters_new,1,getErrorTime,params2,dat=yields_obs,effort,cl=cl)

stopCluster(cl)
#save error
saveRDS(parameters_new,"lhs_paramspace.RDS")
```

Find param set with the lowest error.

```{r,code_folding=TRUE,eval=F}
findmin<-which(parameters_new$error==min(parameters_new$error,na.rm=T))

result<-parameters_new[findmin,]
```

Plug it back into the model and compare the error values.

```{r,code_folding=TRUE,eval=F}
params_lhs<-params2
params_lhs<-setBevertonHolt(params_lhs, reproduction_level = result[1:12])

# check error and compare with original error
(new<-getErrorTime(vary=result[1:12],params=params_lhs,effort,yields_obs,variable = "reproduction_level"))

(original<-getErrorTime(getReproductionLevel(params2), params2,effort, yields_obs,variable = "reproduction_level"))

```

# Optimisation

We could then pass the error function to *fastOptim()*, which uses
*optimParallel* to estimate the lowest sum of squared errors between the
observed and modelled catches for all species' time series. If you want
to see how to write the error function type "getErrorTime" to see what
it contains.

You may want to skip the below step as it could take a while to run.

```{r,code_folding=TRUE,eval=F}
sim_optim<-fastOptim(params=params2,effort=effort,dat=yields_obs,variable = "reproduction_level",lower_val = 0.5,upper_val = 0.999)

# compare the error to the error obtained in the above chunk
(optim_error<-getErrorTime(getReproductionLevel(sim_optim@params), sim_optim@params,effort, yields_obs,variable = "reproduction_level"))

```

Next, we can check how much better the error is and how this affects our
interpretation when we plot the data.

```{r,code_folding=TRUE,eval=F}
# assign these to the param object
#params_optim<-params_lhs
params_optim<-sim_optim@params
params_optim<- projectToSteady(params_optim, effort = effort[1,])
simt_optim<- project(params_optim, effort = effort)

plotYield(simt_optim) +    facet_wrap(~Species,scales = "free")+
geom_point(data=filter(yields_obs),aes(x = Year, y = (Yield), colour = Species),size=0.1)

getReproductionLevel(params_optim)

```

The error hasn't really reduced that much through the parameter space
sampling or optimisation of reproduction_level and the plots do not seem
remarkably different, though improved.

Let's compare this with the more widely used approach of estimating
Rmax, keeping the tuned erepro and reproduction_level values fixed. This
wll change the steady state so we need to re-calculate it.

```{r,code_folding=TRUE,eval=F}
sim_optim_Rmax<-fastOptim(params=params_lhs,effort=effort,dat=yields_obs,variable = "R_max",lower_val = 1,upper_val = 6)

# compare the error to the time-average steady state version
optim_error_Rmax<-getErrorTime(species_params(sim_optim_Rmax@params)$R_max, sim_optim_Rmax@params,effort, yields_obs,sumsquares = F)

original<-getErrorTime(getReproductionLevel(params2), params2,effort, yields_obs,sumsquares = F)
```

Then plot it again.

```{r,code_folding=TRUE,eval=F}
params_optim_Rmax<-sim_optim_Rmax@params
params_optim_Rmax<- projectToSteady(params_optim_Rmax, effort = effort[1,])
simt_optim_Rmax<- project(params_optim_Rmax, effort = effort)

plotYield(simt_optim_Rmax) +  facet_wrap(~Species,scales = "free") +
geom_point(data=filter(yields_obs),aes(x = Year, y = (Yield), colour = Species),size=0.1)
```

# Summary

-   After arriving at a steady state that fit the time-averaged catch
    data pretty well (using matchYields), we needed to set the
    sensitivity to fishing via the level of density dependence in our
    model. Initially we arbitrarily chose a high level- 0.9, but this is
    an important parameter in terms of understanding how stocks collapse
    and recover to fishing and is likely also to be species-specific.

-   We confronted the model with time-series of fishing mortalities and
    catch data. From this we saw that lower values of
    *reproduction_level* resulted in stock collapse earlier in the time
    series.

-   We explored a couple of different approaches to simultaneously
    estimate the reproduction_level for all species in the model. The
    best parameter set given the data revealed very high levels of
    density dependence for all species (except N.pout, Herring, and
    Gurnard).

-   Neither of the estimation methods revealed a pattern in the level of
    density dependence with weight at infinity. It would be interesting
    to know whether or not the modeled catches fit the data better using
    our previous *Rmax* estimation approach.

-   While we did not explore it, the modeled catches are very sensitive
    to the initial fishing mortality rates. My next exploration aims to
    investigate whether we draw similar conclusins when we do not use
    inputs from single-species stock assessments.

```{r}
rl<-data.frame(rl=c(getReproductionLevel(params_lhs), getReproductionLevel(params_optim)),
method=c(rep("lhs",12),rep("optim",12)),
winf=rep(species_params(params_lhs)$w_inf,2))


ggplot(rl,aes(x=log10(winf),y=rl,col=method)) + geom_point() 
```

#### Acknoweldgements

Thank you to Gustav Delius and Asta Audzijonyte for the push to write this blog and the stimulating discussions on this topic.
